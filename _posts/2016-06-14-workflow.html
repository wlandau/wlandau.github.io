---
layout: post
title: "Workflow management in R"
comments: true
root: "../../../../"
---

<h1>Workflow management in R</h1>

<p>
When I seriously analyze data in R, I like to bundle my
materials in a reproducible workflow so that changes during
development are as smooth as possible and so others can easily replicate
the results. Until recently, I built formal R packages around my
work for the sake of portability and quality control,
but I now prefer tools that more strongly emphasize smooth development
and easy replication. These are my thoughts so far, which I explain in
terms of the natural progression of the tools I like.
</p>

<h2><a href="https://www.gnu.org/software/make/">GNU Make</a></h2>

<p>
Programmers who work with compiled languages have been using
<a href="https://www.gnu.org/software/make/">GNU Make</a>
for decades, and you can use it with R. Here's an example that makes
a file called <code>plot.pdf</code>.
 I write the steps to make <code>plot.pdf</code> in a file
called <code>Makefile</code> below.
</p>

<pre><code>
all: plot.pdf

plot.pdf: plot.R random.csv mtcars.csv
	Rscript plot.R

mtcars.csv: mtcars.R
	Rscript mtcars.R

random.csv: random.R
	Rscript random.R

clean:
	rm -f mtcars.csv random.csv plot.pdf
</code></pre>

<p>

Above, <code>plot.pdf</code> depends on <code>plot.R</code>,
<code>random.csv</code>, and <code>mtcars.csv</code>. With those dependencies available,
<code>plot.pdf</code> is constructed by executing <code>Rscript plot.R</code> in the
<a href = "http://linuxcommand.org/">command line</a>. Similarly, <code>mtcars.csv</code>
depends on <code>mtcars.R</code> and is constructed by calling <code>Rscript mtcars.R</code>
in the <a href = "http://linuxcommand.org/">command line</a>. The case of <code>random.csv</code>
is analogous.
</p>

<p>
Alongside the <code>Makefile</code>, I have <code>plot.R</code>,

<pre><code>
x <- read.csv("mtcars.csv")[["mpg"]]
y <- read.csv("random.csv")[["y"]]
pdf("plot.pdf")
plot(x, y)
dev.off()
</code></pre>

<code>mtcars.R</code>,

<pre><code>
data(mtcars)
write.csv(mtcars, "mtcars.csv")
</code></pre>

and <code>random.R</code>.

<pre><code>
d = data.frame(y = rnorm(32))
write.csv(d, "random.csv")
</code></pre>


With all these files, I open a
<a href="https://en.wikipedia.org/wiki/Terminal_(OS_X)">command line program</a>,
 type <code>make</code>, and press enter. The files are generated in sequence,
 and the console shows the following.

<pre><code>
Rscript random.R
Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>
</p>

<p>
If I wanted to distribute this workflow over two parallel processes, I could have typed
 <code>make -j 2</code>. In that case, <code>mtcars.csv</code> and
<code>random.csv</code> would have been computed simultaneously, and then <code>plot.pdf</code>
would have been created afterwards.
</p>

<p>
If I run <code>make</code> again, I get

<pre><code>
make: Nothing to be done for `all'.
</code></pre>

One of the best features of <a href="https://www.gnu.org/software/make/">GNU Make</a>
is that "targets" such as <code>plot.pdf</code>, <code>mtcars.csv</code>, and
<code>random.csv</code> are only (re)built when their dependencies change or need to be generated. For example, suppose
I change <code>mtcars.R</code> and rerun <code>make</code>. Then, <code>mtcars.csv</code> and <code>plot.pdf</code>
are rebuilt, but <code>random.csv</code> is left alone.

<pre><code>
Rscript mtcars.R
Rscript plot.R
null device
          1
</code></pre>
</p>

<p>
<a href="https://www.gnu.org/software/make/">Make</a> saves a lot of time, but
the solution is incomplete. To get the most out of
<a href="https://www.gnu.org/software/make/">Make</a>'s conditional rebuild policy,
you'll have to split up your R code into as many separate files as possible and
collate them yourself, which could get messy. Also, adding comments and whitespace
triggers unnecessary rebuilds. A better solution for R is
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
<a href="https://github.com/richfitz/remake">remake</a>.
</p>


<h2><a href="https://github.com/richfitz/remake">remake</a></h2>

<p>
<a href="https://richfitz.github.io/">Rich FitzJohn</a>'s
 <a href="https://github.com/richfitz/remake">remake</a> package is like
 <a href="https://www.gnu.org/software/make/">GNU Make</a> for a single R session.
To run the previous example, I first create the <a href="http://yaml.org/">YAML</a>
file <code>remake.yml</code> below, which is analogous to a
<a href="https://www.gnu.org/software/make/">Makefile</a>.

<pre><code>
sources: code.R

targets:

  all:
    depends: plot.pdf

  plot.pdf:
    command: my_plot(mtcars, random)
    plot: TRUE

  mtcars:
    command: my_mtcars()

  random:
    command: my_random()
</code></pre>

I also maintain a source file called <code>code.R</code>.

<pre><code>
my_mtcars = function(){
  data(mtcars)
  mtcars
}

my_random = function(){
  data.frame(y = rnorm(32))
}

my_plot = function(mtcars, random){
  plot(mtcars$mpg, random$y)
}
</code></pre>

Then, to build the project, I open an R session and run the following.

<pre><code>
library(remake)
make()
</code></pre>
</p>

<p>
Only out-of-date or missing targets are (re)built. For example, changing
the contents of <code>my_random()</code>
does not trigger a rebuild of <code>mtcars</code>. In addition,
you can add comments and whitespace throughout <code>code.R</code>
without triggering redundant rebuilds. I also like that I don't have
to manage intermediate files. Rather than having to save CSV files,
I can let <a href="https://github.com/richfitz/remake">remake</a>
compute objects <code>mtcars</code> and <code>random</code> and maintain
them in a hidden folder called <code>.remake/objects</code>, managed internally by
<a href="https://github.com/richfitz/storr">storr</a>.
</p>

<p>
<a href="https://github.com/richfitz/remake">Remake</a> is amazing, but it
still has room for improvement. For one, it currently does not support parallel
computing, so there is not a built-in way to distribute targets over parallel processes
as with <code>make -j</code>. In addition, the user needs to maintain a
<a href="http://yaml.org/">YAML</a> file that could get large and complicated for
serious projects. With these issues in mind, I wrote the
<a href="https://github.com/wlandau/parallelRemake">parallelRemake</a> and
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
packages.
</p>

<h2><a href="https://github.com/wlandau/parallelRemake">parallelRemake</a></h2>

<p>
The <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>
package is basically <a href="https://github.com/richfitz/remake">remake</a>
plus parallel computing. You begin with a <a href="http://yaml.org/">YAML</a>
file such as <code>remake.yml</code> as in the above example, and then you
write a <a href="https://www.gnu.org/software/make/">Makefile</a>
that can run <a href="https://github.com/richfitz/remake">remake</a>
targets in parallel. Using the previous
example, I first open an R session and call <code>write_makefile()</code>.

<pre><code>
library(parallelRemake)
write_makefile()
</code></pre>

That produces the proper <a href="https://www.gnu.org/software/make/">Makefile</a>
shown below.

<pre><code>
# Generated by parallelRemake::write_makefile() on 2016-06-14 08:45:47

.PHONY: all plot.pdf mtcars random clean

all: plot.pdf

plot.pdf: mtcars random
	Rscript -e 'remake::make("plot.pdf", remake_file = "remake.yml")'

mtcars:
	Rscript -e 'remake::make("mtcars", remake_file = "remake.yml")'

random:
	Rscript -e 'remake::make("random", remake_file = "remake.yml")'

clean:
	Rscript -e 'remake::make("clean", remake_file = "remake.yml")'
	rm -rf .remake
</code></pre>
</p>

<p>
I can now run the whole project in two parallel processes with
<code>make -j 2</code>, and the targets <code>mtcars</code>
and <code>random</code> are built in parallel. Thus, we have
parallel computing plus all the original advantages of
<a href="https://github.com/richfitz/remake">remake</a>.
</p>

<p>
You can set up and run this example from start to finish with
<pre><code>
library(parallelRemake)
run_example_parallelRemake()
</code></pre>
</p>

<p>
As before, intermediate R objects are stored in <code>.remake/objects</code>,
a cache managed internally by <a href="https://github.com/richfitz/storr">storr</a>.
To load these objects into an R session for debugging and planning purposes,
use the <code>recall()</code> function. To see the objects available,
use <code>recallable()</code>.
</p>

<h2><a href="https://github.com/wlandau/workflowHelper">workflowHelper</a></h2>

<p>
This package is for anyone who uses R to analyze
multiple datasets in multiple ways. For a common set of use cases,
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
 makes it much easier to set up and maintain
a reproducible workflow in the midst of heavy development. The user supplies functions to
generate each dataset and perform each analysis, and then the package
puts all the pieces together and manages the files for you. Let's dive into an
example, which you can run from start to finish with

<pre><code>
library(workflowHelper)
run_example_workflowHelper()
</code></pre>

The online file <a href="https://github.com/wlandau/workflowHelper/blob/master/inst/example/code.R"><code>code.R</code></a>
has all the building blocks of the workflow.
In practice, you can divide <code>code.R</code> into separate R files
however you want. The file online
<a href="https://github.com/wlandau/workflowHelper/blob/master/inst/example/workflow.R"><code>workflow.R</code></a>,
also below, structures the workflow.

<code><pre>
library(workflowHelper)

sources = strings(code.R)
packages = "rmarkdown"

datasets = commands(
  poisson100 = poisson_dataset(n = 100),
  normal100 = normal_dataset(n = 100),
  normal1000 = normal_dataset(n = 1000)
)

# For 4 replicates of each kind of dataset,
# assign datasets = reps(datasets, 4)

analyses = commands(
  lm = lm_analysis(..dataset..),
  glm = glm_analysis(..dataset..)
)

summaries = commands(
  mse = mse_summary(..dataset.., ..analysis..),
  coef = coef_summary(..analysis..)
)

output = commands(
  coef_table = do.call(I("rbind"), coef),
  coef.csv = write.csv(coef_table, target_name),
  mse_vector = unlist(mse)
)

plots = commands(
  mse.pdf = hist(mse_vector, col = I("black"))
)

reports = commands(
  report.md = list(fig.height = 7, fig.align = "right"),
  report.html = render("report.md", quiet = TRUE)
)

begin = c("# This is my Makefile", "# Variables...")
plan_workflow(sources, packages, datasets, analyses, summaries,
  output, plots, reports = reports, begin = begin)
</code></pre>
</p>

<p>
Using the <code>..dataset..</code> and <code>..analysis..</code>
wildcards, a plan is created to analyze each dataset with each method,
 summarize each analysis of each dataset, and then produce general output,
 plots, and reports. The package automatically assumes <code>report.md</code>
 is knitted from <code>report.Rmd</code> using <a href="http://yihui.name/knitr/">knitr</a>.
Here, either a list of <a href="http://yihui.name/knitr/">knitr</a> options or <code>TRUE</code>
 is given in place of a command.
The function <code>plan_workflow()</code> at the bottom of <code>workflow.R</code> generates a
<a href="http://yaml.org/">YAML</a> file with instructions to
<a href="https://github.com/richfitz/remake">remake</a> and then produces an overarching
<a href="https://www.gnu.org/software/make/">Makefile</a>
using <code>parallelRemake::write_makefile()</code>. If you do not need any parallel
computing, you can control the workflow with <code>remake::make</code> in a single
R session.

<code><pre>
library(remake)
make(remake_file = "remake.yml")
</code></pre>

But if you want to distrubite the workflow over multiple simultaneous
processes, use the <a href="https://www.gnu.org/software/make/">Makefile</a> and run the project with
<code>make -j [n]</code>, where <code>[n]</code> is the number of
parallel processes. The main advantages of <a href="https://github.com/richfitz/remake">remake</a>
and <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>,
such as conditional rebuilds, remain.
</p>

<p>
Datasets, analyses, and summaries are stored in the <code>.remake/objects</code>
<a href="https://github.com/richfitz/storr">storr</a> cache.
To read and inspect them, use <code>recall()</code> and
 <code>recallable()</code> as with <a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>.
 For serious iterations of your workflow, however, it is best not to manually access this cache
 because changes there are not tracked.
</p>

<h2><a href="https://github.com/wlandau/downsize">downsize</a></h2>

<p>
When I'm setting up a reproducible workflow in R, I don't run the
full scaled-up workflow right away. I first run a downsized
version to test and debug. I wrote the <a href="https://github.com/wlandau/downsize">downsize</a>
 package to help with the downsizing.
It can be used to shrink the size of datasets, the number of
Monte Carlo iterations to run, etc.
With the <code>ds</code> function, objects are downsized if
<code>getOption("downsize")</code> is <code>TRUE</code> and left alone otherwise.
Here are some examples. Try running these lines by themselves first. Then,
enter <code>options(downsize = TRUE)</code> and run them again to see what changes.

<code><pre>
library(downsize)
ds("Leave me alone when downsize is FALSE.", "Return me when downsize is TRUE.")
ds(1:10, length = 2)
m = matrix(1:36, ncol = 6)
ds(m, ncol = 2)
ds(m, ncol = 2, random = T)
ds(m, nrow = 2)
ds(m, dim = c(2, 2))
ds(data.frame(x = 1:10, y = 1:10), nrow = 5)
dim(ds(array(0, dim = c(10, 100, 2, 300, 12)), dim = rep(3, 5)))
</code></pre>

For atomic objects, setting <code>random</code> to <code>TRUE</code>
takes a random subset of elements instead of simply the first few.
</p>

<p>
To use <a href="https://github.com/wlandau/downsize">downsize</a>
with the <a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>
example, just include <code>downsize</code> in
<code>packages</code> inside <code>workflow.R</code> and
replace the top few lines of <code>code.R</code> with the following.

<code><pre>
options(downsize = TRUE)

poisson_dataset = function(n = 100){
  ds(data.frame(x = rpois(n, 1), y = rpois(n, 5)), nrow = 10)
}

normal_dataset = function(n = 100){
  ds(data.frame(x = rnorm(n, 1), y = rnorm(n, 5)), nrow = 10)
}
</code></pre>

This sets the <code>downsize</code> option and use the <code>ds</code>
function to shrink the datasets. For the full scaled-up workflow,
just replace the first line with <code>options(downsize = FALSE)</code>.
Unfortunately, <a href="https://github.com/richfitz/remake">remake</a>
 does not rebuild things when options are changed,
 so you'll have to run <code>make clean</code> whenever you
 change the <code>downsize</code> option.
</p>

<h2>A final note on workflows as R packages</h2>

<p>
Personally, I would still like to be able to write my workflows as R
packages without abandoning <a href="https://github.com/richfitz/remake">remake</a>.
I like the portability, convenience, transparency, and quality control, and
<a href="http://r-pkgs.had.co.nz/tests.html">automated testing</a> of formal packages.
Unfortunately, <a href="https://github.com/richfitz/remake">remake</a> and custom
packages do not play nicely yet, as I show <a href="https://github.com/wlandau/remakeInPackage">here</a>.
For now, though, I find it worthwhile to switch to
<a href="https://github.com/richfitz/remake">remake</a>,
<a href="https://github.com/wlandau/parallelRemake">parallelRemake</a>, and
<a href="https://github.com/wlandau/workflowHelper">workflowHelper</a>.
</p>
